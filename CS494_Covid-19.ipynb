{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS494 - Colab 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557"
      },
      "source": [
        "CS494 - Colab 3\n",
        "## Studying COVID-19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUUjUvXe3Sjk"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the files we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRElWs_x2mGh"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHsFTGUy2n1c"
      },
      "source": [
        "# Updated 4/22\n",
        "id='1jOcwW4RWnOvZl563-L2xJF0RiAA6lryf'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('time_series_19-covid-Confirmed.csv')\n",
        "\n",
        "id='1xvbVjr22XbKUXv_Lhts8MOSpFc_825Ko'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('time_series_19-covid-Deaths.csv')\n",
        "\n",
        "id='1Xktj9-B4yCaJyKKWnxnwAzYQ9eFhRR50'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('time_series_19-covid-Recovered.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "If you executed the cells above, you should be able to see the dataset we will use for this Colab under the \"Files\" tab on the left panel.\n",
        "\n",
        "Next, we import some of the common libraries needed for our task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqovskkH1DmC"
      },
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DueQggJc1DDk"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3sAVeK1EDZ"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iid7lXcG1CY8"
      },
      "source": [
        "If you are running this Colab on the Google hosted runtime, the cell below will create a *ngrok* tunnel which will allow you to still check the Spark UI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDnGLVPH1BPQ"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqM2vdiVJGpl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_1VVaw6JYvN"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXdMR6wnEIM"
      },
      "source": [
        "In this Colab, we will be analyzing the timeseries data of the Coronavirus COVID-19 Global Cases, collected by Johns Hopkins CSSE.\n",
        "\n",
        "Here you can check a realtime dashboard based on this dataset: [https://www.arcgis.com/apps/opsdashboard/index.html?fbclid=IwAR2hQKsEZ3D38wVtXGryUhP9CG0Z6MYbUM_boPEaV8FBe71wUvDPc65ZG78#/bda7594740fd40299423467b48e9ecf6](https://www.arcgis.com/apps/opsdashboard/index.html?fbclid=IwAR2hQKsEZ3D38wVtXGryUhP9CG0Z6MYbUM_boPEaV8FBe71wUvDPc65ZG78#/bda7594740fd40299423467b48e9ecf6)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   ```confirmed```: dataframe containing the total number of confirmed COVID-19 cases per day, divided by geographical area\n",
        "*   ```deaths```: dataframe containing the number of deaths per day due to COVID-19, divided by geographical area\n",
        "*   ```recovered```: dataframe containing the number of recoevered patients per day, divided by geographical area\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUSeIQlNqv6J"
      },
      "source": [
        "confirmed = spark.read.csv('time_series_19-covid-Confirmed.csv', header=True)\n",
        "deaths = spark.read.csv('time_series_19-covid-Deaths.csv', header=True)\n",
        "recovered = spark.read.csv('time_series_19-covid-Recovered.csv', header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvyFsE-fLXpg"
      },
      "source": [
        "confirmed.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV45mAMfrmxA"
      },
      "source": [
        "### Your Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLLbq024rr9l"
      },
      "source": [
        "We are aware of the great deal of stress we are all experiencing because of the spread of the Coronavirus. As such, we decided to conclude our series of Colabs with a **lightweight task** -- given everything you have learned about Spark during the quarter, this Colab should take you just a few minutes to complete.\n",
        "\n",
        "At the same time, we turned this into an opportunity to raise awareness about the extent of the COVID-19 epidemic.\n",
        "\n",
        "[Stay healthy, wash your hands often](https://www.cdc.gov/coronavirus/2019-ncov/about/index.html), and invest the time you saved on this Colab to be emotionally supportive to your friends and family!\n",
        "\n",
        "Signed, *the CS494 teaching staff*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccei4wpRzcXr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "df2c2d2b-0528-477b-a3cb-586194f46f5a"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "#this code counts how many confirmed, death and recovered are in the last update \n",
        "countConfirmed = confirmed.agg(sum(\"4/22/20\").alias(\"Confirmed\"))\n",
        "countDeaths = deaths.agg(sum(\"4/22/20\").alias(\"Deaths\"))\n",
        "countRecovered = recovered.agg(sum(\"4/22/20\").alias(\"Recovered\"))\n",
        "\n",
        "countConfirmed.show()\n",
        "countDeaths.show()\n",
        "countRecovered.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|Confirmed|\n",
            "+---------+\n",
            "|2624089.0|\n",
            "+---------+\n",
            "\n",
            "+--------+\n",
            "|  Deaths|\n",
            "+--------+\n",
            "|183064.0|\n",
            "+--------+\n",
            "\n",
            "+---------+\n",
            "|Recovered|\n",
            "+---------+\n",
            "| 709694.0|\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wYiCEv_zhVf"
      },
      "source": [
        "Consider only the most recent data points in the timeseries, and compute:\n",
        "\n",
        "\n",
        "*   number of confirmed COVID-19 cases across the globe\n",
        "*   number of deaths due to COVID-19 across the globe\n",
        "*   number of recovered patients across the globe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Hoy-9Xzf8r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "3bd6f15b-d22c-450a-a072-1248235243d4"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "#confirmed less than 50 cases\n",
        "cases = confirmed.withColumn(\"4/22/20\", confirmed[\"4/22/20\"].cast(IntegerType()))\n",
        "sortConfirmed = cases.groupby(\"Country/Region\").sum(\"4/22/20\")\n",
        "cases = sortConfirmed.filter(sortConfirmed[\"sum(4/22/20)\"] >= 50)\n",
        "cases.show(10)\n",
        "casesByCountry = [str(row['Country/Region']) for row in cases.collect()]\n",
        "confirmedCount = cases.agg(sum(\"sum(4/22/20)\").alias(\"Confirmed\"))\n",
        "confirmedCount.show()\n",
        "\n",
        "#recovered cases\n",
        "casesRecovered = recovered.filter(col(\"Country/Region\").isin(casesByCountry))\n",
        "countRecovered = casesRecovered.agg(sum(\"4/22/20\").alias(\"Recovered\"))\n",
        "countRecovered.show()\n",
        "totalRecovered = countRecovered.select(\"*\").toPandas()\n",
        "confirmedCount = confirmedCount.select(\"*\").toPandas()\n",
        "\n",
        "#ratio between confirmed and recovered\n",
        "ratio = totalRecovered[\"Recovered\"][0]/confirmedCount[\"Confirmed\"][0]\n",
        "ratio \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+------------+\n",
            "|Country/Region|sum(4/22/20)|\n",
            "+--------------+------------+\n",
            "|      Paraguay|         213|\n",
            "|        Russia|       57999|\n",
            "|       Senegal|         442|\n",
            "|    Cabo Verde|          73|\n",
            "|        Sweden|       16004|\n",
            "|        Guyana|          67|\n",
            "|   Philippines|        6710|\n",
            "|         Burma|         123|\n",
            "|      Djibouti|         974|\n",
            "|      Malaysia|        5532|\n",
            "+--------------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------+\n",
            "|Confirmed|\n",
            "+---------+\n",
            "|  2623413|\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|Recovered|\n",
            "+---------+\n",
            "| 709523.0|\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27045798736226434"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yahkAHoS0HuU"
      },
      "source": [
        "Consider only the most recent data points in the timeseries, and filter out the geographical locations where less than 50 cases have been confirmed.\n",
        "For the areas still taken into consideration after the filtering step, compute the ratio between number of recovered patients and number of confirmed cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FwzsH4l1VCZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "f2d2fdbb-373b-4ed9-d3da-356840ee01e9"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "#number of deaths \n",
        "deathCount = deaths.filter(col(\"Country/Region\").isin(casesByCountry))\n",
        "totalDeaths = deathCount.agg(sum(\"4/22/20\").alias(\"Deaths\"))\n",
        "totalDeaths.show()\n",
        "totalDeaths = totalDeaths.select(\"*\").toPandas()\n",
        "#ratio between number of deaths and number of confirmed cases\n",
        "ratio = totalDeaths[\"Deaths\"][0]/confirmedCount[\"Confirmed\"][0]\n",
        "ratio \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|  Deaths|\n",
            "+--------+\n",
            "|183037.0|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06977056224086714"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOq8Xd7O1ZJk"
      },
      "source": [
        "Following the same filtering strategy as above, now compute the ratio between number of deaths and number of confirmed cases.\n",
        "\n",
        "**Optional:** Create a global map of the ratio between recovered / confirmed, recovered / deaths, and confirmed / deaths in the final data points in the timeseries using a mapping tool such as Leaflet.js (https://leafletjs.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkW9x36akc0G"
      },
      "source": [
        "# YOUR CODE HERE (Optional)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrXJyVNP2AI"
      },
      "source": [
        "Once you have working code for each cell above, **head over to Gradescope, read carefully the questions, and submit your solution for this Colab**!"
      ]
    }
  ]
}